{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2409d98c-78f7-4420-80aa-d0bc82a6269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: object (350,)\n",
      "Splitting 350 items into parts <= 100.0 MB\n",
      "Output dir: C:\\Users\\user\\Desktop\\data\\moving_target_dataset_parts\n",
      "Saved moving_target_dataset.part0000.npy: items 0:114 (96.7 MB)\n",
      "Saved moving_target_dataset.part0001.npy: items 114:133 (96.5 MB)\n",
      "Saved moving_target_dataset.part0002.npy: items 133:164 (98.4 MB)\n",
      "Saved moving_target_dataset.part0003.npy: items 164:171 (72.9 MB)\n",
      "Saved moving_target_dataset.part0004.npy: items 171:172 (42.9 MB)\n",
      "Saved moving_target_dataset.part0005.npy: items 172:173 (75.6 MB)\n",
      "Saved moving_target_dataset.part0006.npy: items 173:175 (71.4 MB)\n",
      "Saved moving_target_dataset.part0007.npy: items 175:191 (80.2 MB)\n",
      "Saved moving_target_dataset.part0008.npy: items 191:203 (89.2 MB)\n",
      "Saved moving_target_dataset.part0009.npy: items 203:210 (94.2 MB)\n",
      "Saved moving_target_dataset.part0010.npy: items 210:234 (98.7 MB)\n",
      "Saved moving_target_dataset.part0011.npy: items 234:253 (98.0 MB)\n",
      "Saved moving_target_dataset.part0012.npy: items 253:268 (91.2 MB)\n",
      "Saved moving_target_dataset.part0013.npy: items 268:283 (95.0 MB)\n",
      "Saved moving_target_dataset.part0014.npy: items 283:311 (99.4 MB)\n",
      "Saved moving_target_dataset.part0015.npy: items 311:333 (99.8 MB)\n",
      "Saved moving_target_dataset.part0016.npy: items 333:350 (52.5 MB)\n",
      "Done ✅ Created 17 files\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# =======================\n",
    "# USTAWIENIA\n",
    "# =======================\n",
    "INPUT_PATH = Path(\"moving_target_dataset.npy\")\n",
    "OUT_DIR = Path(f\"{INPUT_PATH.stem}_parts\")\n",
    "MAX_MB = 100.0\n",
    "\n",
    "# =======================\n",
    "# WCZYTANIE\n",
    "# =======================\n",
    "arr = np.load(INPUT_PATH, allow_pickle=True)\n",
    "print(\"Loaded:\", arr.dtype, arr.shape)\n",
    "\n",
    "if not (isinstance(arr, np.ndarray) and arr.dtype == object and arr.ndim == 1):\n",
    "    raise ValueError(f\"Oczekuję 1D ndarray dtype=object. Mam: dtype={arr.dtype}, shape={arr.shape}, ndim={arr.ndim}\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "max_bytes = int(MAX_MB * 1024 * 1024)\n",
    "base = INPUT_PATH.stem\n",
    "\n",
    "def chunk_size_bytes(chunk: np.ndarray) -> int:\n",
    "    \"\"\"Zapisuje chunk do pliku tymczasowego i zwraca jego rozmiar w bajtach.\"\"\"\n",
    "    fd, tmp_path = tempfile.mkstemp(suffix=\".npy\")\n",
    "    os.close(fd)\n",
    "    try:\n",
    "        np.save(tmp_path, chunk, allow_pickle=True)\n",
    "        return Path(tmp_path).stat().st_size\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(tmp_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "# =======================\n",
    "# DZIELENIE BEZ NADPISYWANIA\n",
    "# =======================\n",
    "n = len(arr)\n",
    "part_idx = 0\n",
    "start = 0\n",
    "\n",
    "print(f\"Splitting {n} items into parts <= {MAX_MB} MB\")\n",
    "print(\"Output dir:\", OUT_DIR.resolve())\n",
    "\n",
    "current_start = 0\n",
    "current_end = 0  # exclusive\n",
    "\n",
    "while current_end < n:\n",
    "    # spróbujmy powiększyć chunk o jeden element\n",
    "    candidate_end = current_end + 1\n",
    "    candidate = arr[current_start:candidate_end]\n",
    "\n",
    "    size = chunk_size_bytes(candidate)\n",
    "\n",
    "    if size <= max_bytes:\n",
    "        # OK — przyjmujemy większy chunk\n",
    "        current_end = candidate_end\n",
    "        continue\n",
    "\n",
    "    # Jeśli nie mieści się, to zapisujemy \"poprzedni\" chunk\n",
    "    if current_end == current_start:\n",
    "        # Nawet 1 element jest za duży -> zapisujemy go osobno (będzie > limit)\n",
    "        single = arr[current_start:current_start + 1]\n",
    "        out_path = OUT_DIR / f\"{base}.part{part_idx:04d}.npy\"\n",
    "        np.save(out_path, single, allow_pickle=True)\n",
    "        out_size = out_path.stat().st_size\n",
    "        print(f\"[WARN] Single item > limit -> {out_path.name} ({out_size/1024/1024:.1f} MB)\")\n",
    "\n",
    "        part_idx += 1\n",
    "        current_start += 1\n",
    "        current_end = current_start\n",
    "        continue\n",
    "\n",
    "    # zapisujemy chunk, który się mieścił (current_start:current_end)\n",
    "    chunk = arr[current_start:current_end]\n",
    "    out_path = OUT_DIR / f\"{base}.part{part_idx:04d}.npy\"\n",
    "    np.save(out_path, chunk, allow_pickle=True)\n",
    "    out_size = out_path.stat().st_size\n",
    "    print(f\"Saved {out_path.name}: items {current_start}:{current_end} ({out_size/1024/1024:.1f} MB)\")\n",
    "\n",
    "    part_idx += 1\n",
    "    current_start = current_end\n",
    "    # current_end zostaje, bo zaczynamy nowy chunk od current_start\n",
    "\n",
    "# zapis ostatniego chunku (jeśli coś zostało)\n",
    "if current_end > current_start:\n",
    "    chunk = arr[current_start:current_end]\n",
    "    out_path = OUT_DIR / f\"{base}.part{part_idx:04d}.npy\"\n",
    "    np.save(out_path, chunk, allow_pickle=True)\n",
    "    out_size = out_path.stat().st_size\n",
    "    print(f\"Saved {out_path.name}: items {current_start}:{current_end} ({out_size/1024/1024:.1f} MB)\")\n",
    "    part_idx += 1\n",
    "\n",
    "print(f\"Done ✅ Created {part_idx} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "738bda0b-d44f-4f25-8059-7c4fd939975f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 part files in: C:\\Users\\user\\Desktop\\data\\moving_target_dataset_parts\n",
      "First: moving_target_dataset.part0000.npy\n",
      "Last : moving_target_dataset.part0016.npy\n",
      "Merged dtype/shape: object (350,)\n",
      "Merged length: 350 Expected: 350\n",
      "Saved merged file: C:\\Users\\user\\Desktop\\data\\moving_target_dataset_merged.npy (1452.7 MB)\n",
      "Reload test: OK ✅\n",
      "\n",
      "Verifying against original: C:\\Users\\user\\Desktop\\data\\moving_target_dataset.npy\n",
      "Length match: OK ✅\n",
      "Sample content check (20 items): OK ✅\n",
      "Dict keys check: OK ✅\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# =======================\n",
    "# USTAWIENIA\n",
    "# =======================\n",
    "PARTS_DIR = Path(\"moving_target_dataset_parts\")     # folder z częściami\n",
    "OUTPUT_PATH = Path(\"moving_target_dataset_merged.npy\")\n",
    "\n",
    "# opcjonalnie: oryginał do weryfikacji 1:1 (zostaw None jeśli nie chcesz / nie masz)\n",
    "ORIGINAL_PATH = Path(\"moving_target_dataset.npy\")   # albo None\n",
    "\n",
    "# ile losowych rekordów porównać (im więcej, tym pewniej, ale wolniej)\n",
    "SAMPLE_CHECKS = 20\n",
    "SEED = 123\n",
    "\n",
    "# =======================\n",
    "# POMOCNICZE\n",
    "# =======================\n",
    "def stable_item_digest(obj) -> str:\n",
    "    \"\"\"Stabilny skrót obiektu po pickle (do porównania rekordów).\"\"\"\n",
    "    b = pickle.dumps(obj, protocol=4)  # tylko do porównania w RAM\n",
    "    return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "# =======================\n",
    "# 1) ZNAJDŹ CZĘŚCI\n",
    "# =======================\n",
    "parts = sorted(PARTS_DIR.glob(\"*.part*.npy\"))\n",
    "if not parts:\n",
    "    raise FileNotFoundError(f\"Nie znaleziono plików *.part*.npy w: {PARTS_DIR.resolve()}\")\n",
    "\n",
    "print(f\"Found {len(parts)} part files in: {PARTS_DIR.resolve()}\")\n",
    "print(\"First:\", parts[0].name)\n",
    "print(\"Last :\", parts[-1].name)\n",
    "\n",
    "# =======================\n",
    "# 2) WCZYTAJ I SCAL\n",
    "# =======================\n",
    "arrays = []\n",
    "total_len = 0\n",
    "\n",
    "for p in parts:\n",
    "    a = np.load(p, allow_pickle=True)\n",
    "    if not (isinstance(a, np.ndarray) and a.dtype == object and a.ndim == 1):\n",
    "        raise ValueError(f\"Zły format w {p.name}: dtype={getattr(a,'dtype',None)}, shape={getattr(a,'shape',None)}\")\n",
    "    arrays.append(a)\n",
    "    total_len += len(a)\n",
    "\n",
    "merged = np.concatenate(arrays, axis=0)\n",
    "print(\"Merged dtype/shape:\", merged.dtype, merged.shape)\n",
    "print(\"Merged length:\", len(merged), \"Expected:\", total_len)\n",
    "\n",
    "if len(merged) != total_len:\n",
    "    raise AssertionError(\"Długość merged nie zgadza się z sumą długości części!\")\n",
    "\n",
    "# =======================\n",
    "# 3) ZAPISZ\n",
    "# =======================\n",
    "np.save(OUTPUT_PATH, merged, allow_pickle=True)\n",
    "print(\"Saved merged file:\", OUTPUT_PATH.resolve(), f\"({OUTPUT_PATH.stat().st_size/1024/1024:.1f} MB)\")\n",
    "\n",
    "# =======================\n",
    "# 4) TEST: CZY DA SIĘ ODCZYTAĆ\n",
    "# =======================\n",
    "reloaded = np.load(OUTPUT_PATH, allow_pickle=True)\n",
    "assert reloaded.dtype == object and reloaded.ndim == 1 and len(reloaded) == len(merged)\n",
    "print(\"Reload test: OK ✅\")\n",
    "\n",
    "# =======================\n",
    "# 5) TEST: WERYFIKACJA Z ORYGINAŁEM (opcjonalnie, ale najlepsza)\n",
    "# =======================\n",
    "if ORIGINAL_PATH is not None and Path(ORIGINAL_PATH).exists():\n",
    "    print(\"\\nVerifying against original:\", Path(ORIGINAL_PATH).resolve())\n",
    "    orig = np.load(ORIGINAL_PATH, allow_pickle=True)\n",
    "\n",
    "    if not (isinstance(orig, np.ndarray) and orig.dtype == object and orig.ndim == 1):\n",
    "        raise ValueError(f\"Oryginał ma zły format: dtype={orig.dtype}, shape={orig.shape}\")\n",
    "\n",
    "    assert len(orig) == len(merged), f\"Różna liczba elementów: orig={len(orig)} merged={len(merged)}\"\n",
    "    print(\"Length match: OK ✅\")\n",
    "\n",
    "    random.seed(SEED)\n",
    "    idxs = random.sample(range(len(orig)), k=min(SAMPLE_CHECKS, len(orig)))\n",
    "\n",
    "    mismatches = []\n",
    "    for i in idxs:\n",
    "        d1 = stable_item_digest(orig[i])\n",
    "        d2 = stable_item_digest(merged[i])\n",
    "        if d1 != d2:\n",
    "            mismatches.append(i)\n",
    "\n",
    "    if mismatches:\n",
    "        raise AssertionError(f\"Mismatch w rekordach na indeksach: {mismatches[:10]} (pokazuję max 10)\")\n",
    "    print(f\"Sample content check ({len(idxs)} items): OK ✅\")\n",
    "\n",
    "    # dodatkowy sanity check: czy klucze dictów się zgadzają (jeśli rekordy są dictami)\n",
    "    if isinstance(orig[0], dict) and isinstance(merged[0], dict):\n",
    "        assert orig[0].keys() == merged[0].keys()\n",
    "        print(\"Dict keys check: OK ✅\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n[INFO] ORIGINAL_PATH nie ustawiony albo plik nie istnieje — pomijam porównanie 1:1.\")\n",
    "    print(\"Masz wciąż test 'Reload OK' + sumę długości części.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
